
import logging
import argparse
import sys
from typing import List

import hydra

import lightning as L
from omegaconf import DictConfig, OmegaConf, open_dict
from lightning.pytorch.loggers import Logger
from perturbench.modelcore.utils import multi_instantiate
from hydra.core.hydra_config import HydraConfig

log = logging.getLogger(__name__)


_PARSER_ARGS = None


def _str2bool(v: str | bool | None) -> bool | None:
    if v is None or isinstance(v, bool):
        return v
    if isinstance(v, str):
        if v.lower() in {"true", "1", "yes", "y"}:
            return True
        if v.lower() in {"false", "0", "no", "n"}:
            return False
    return None


def _build_arg_parser() -> argparse.ArgumentParser:
    """
    Parse command line flags (e.g. from wandb sweep) and later override Hydra config.
    We deliberately keep these as standard --flags so that wandb can inject them.
    """
    parser = argparse.ArgumentParser(add_help=False)

    # config groups (will be translated into Hydra overrides like data=..., model=...)
    parser.add_argument("--data", dest="data", type=str)
    parser.add_argument("--model", dest="model", type=str)
    parser.add_argument("--logger", dest="logger", type=str)

    # leaf parameters that we want to override directly
    parser.add_argument("--data.task", dest="data_task", type=str)
    parser.add_argument("--data.data_path", dest="data_data_path", type=str)

    parser.add_argument("--train", dest="train", type=str)
    parser.add_argument("--test", dest="test", type=str)
    parser.add_argument("--test_ckpt_type", dest="test_ckpt_type", type=str)

    parser.add_argument(
        "--trainer.log_every_n_steps",
        dest="trainer_log_every_n_steps",
        type=int,
    )
    parser.add_argument("--trainer.max_epochs", dest="trainer_max_epochs", type=int)
    parser.add_argument("--trainer.min_epochs", dest="trainer_min_epochs", type=int)
    # devices is usually a Hydra list, keep it as string so Hydra can parse it
    parser.add_argument("--trainer.devices", dest="trainer_devices", type=str)

    parser.add_argument("--data.val_batch_size", dest="data_val_batch_size", type=int)
    parser.add_argument(
        "--data.test_batch_size",
        dest="data_test_batch_size",
        type=int,
    )
    parser.add_argument(
        "--data.transform.use_covs",
        dest="data_transform_use_covs",
        type=str,
    )

    parser.add_argument("--model.use_mask", dest="model_use_mask", type=str)
    parser.add_argument("--model.use_cell_emb", dest="model_use_cell_emb", type=str)
    parser.add_argument("--data.train_batch_size", dest="data_train_batch_size", type=int)
    parser.add_argument("--model.dropout", dest="model_dropout", type=float)
    parser.add_argument("--model.lr", dest="model_lr", type=float)
    parser.add_argument(
        "--model.lr_scheduler_max_lr",
        dest="model_lr_scheduler_max_lr",
        type=float,
    )
    parser.add_argument("--model.wd", dest="model_wd", type=float)

    # logger / wandb related
    parser.add_argument(
        "--logger.wandb.project",
        dest="logger_wandb_project",
        type=str,
    )
    parser.add_argument(
        "--logger.wandb.name",
        dest="logger_wandb_name",
        type=str,
    )

    return parser


def _apply_cli_overrides(cfg: DictConfig, args: argparse.Namespace | None) -> DictConfig:
    """Override Hydra cfg using highest-priority CLI arguments."""
    if args is None:
        return cfg

    # Allow adding new keys via CLI (e.g., data.task, model.lr) even when struct is enabled.
    # We only relax struct during the override phase.
    from omegaconf import OmegaConf as _OC
    _OC.set_struct(cfg, False)

    mapping: dict[str, str] = {
        # config groups are handled via Hydra overrides in __main__, not here:
        # "data": "data",
        # "model": "model",
        # "logger": "logger",
        "data_task": "data.task",
        "data_data_path": "data.data_path",
        "train": "train",
        "test": "test",
        "test_ckpt_type": "test_ckpt_type",
        "trainer_log_every_n_steps": "trainer.log_every_n_steps",
        "trainer_max_epochs": "trainer.max_epochs",
        "trainer_min_epochs": "trainer.min_epochs",
        # "trainer_devices" is translated to a Hydra override in __main__
        "data_val_batch_size": "data.val_batch_size",
        "data_test_batch_size": "data.test_batch_size",
        "data_transform_use_covs": "data.transform.use_covs",
        "model_use_mask": "model.use_mask",
        "model_use_cell_emb": "model.use_cell_emb",
        "data_train_batch_size": "data.train_batch_size",
        "model_dropout": "model.dropout",
        "model_lr": "model.lr",
        "model_lr_scheduler_max_lr": "model.lr_scheduler_max_lr",
        "model_wd": "model.wd",
        "logger_wandb_project": "logger.wandb.project",
        "logger_wandb_name": "logger.wandb.name",
    }

    for attr, key in mapping.items():
        if not hasattr(args, attr):
            continue
        value = getattr(args, attr)
        if value is None:
            continue

        # Convert string booleans for known boolean fields
        if key in {
            "train",
            "test",
            "data.transform.use_covs",
            "model.use_mask",
            "model.use_cell_emb",
        }:
            value = _str2bool(value)
            if value is None:
                continue
        OmegaConf.update(cfg, key, value, merge=False)

    return cfg


def train(runtime_context: dict):

    cfg = runtime_context["cfg"]
    # Set seed for random number generators in pytorch, numpy and python.random
    if cfg.get("seed"):
        L.seed_everything(cfg.seed, workers=True)

    log.info("Instantiating datamodule <%s>", cfg.data._target_)
    datamodule: L.LightningDataModule =hydra.utils.instantiate(
        cfg.data,
        seed=cfg.seed
    ) # 初始化cfg.data['_target_']对应的类，并返回一个实例

    log.info("Instantiating model <%s>", cfg.model._target_)
    model = hydra.utils.instantiate(cfg.model, datamodule=datamodule) # 初始化cfg.model['_target_']对应的类，并返回一个实例

    log.info("Instantiating callbacks...")
    callbacks: List[L.Callback] = multi_instantiate(cfg.get("callbacks"))

    log.info("Instantiating loggers...")
    loggers: List[Logger] = multi_instantiate(cfg.get("logger"))

    log.info("Instantiating trainer <%s>", cfg.trainer._target_)
    trainer: L.Trainer = hydra.utils.instantiate(
        cfg.trainer, callbacks=callbacks, logger=loggers
    )

    if cfg.get("train"):
        log.info("Starting training!")
        trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))

    train_metrics = trainer.callback_metrics

    summary_metrics_dict = {}
    if cfg.get("test"):
        log.info("Starting testing!")
        # Track which checkpoint type is actually used for testing so that
        # prediction file names can include this as a suffix.
        selected_ckpt_type = "unknown"
        if cfg.get("train"):
            # if (
            #     trainer.checkpoint_callback is None
            #     or trainer.checkpoint_callback.best_model_path == ""
            # ):
            #     ckpt_path = None
            # else:
            #     ckpt_path = "best"
            # Support multiple checkpoint callbacks (e.g., loss and PCC)
            test_ckpt_type = cfg.get("test_ckpt_type", "loss")  # "loss" or "pcc"
            # Find the appropriate checkpoint callback
            ckpt_path = None
            if callbacks:
                for callback in callbacks:
                    if isinstance(callback, L.pytorch.callbacks.ModelCheckpoint):
                        # Check if this is the checkpoint we want
                        if test_ckpt_type == "loss" and "loss" in str(callback.monitor).lower():
                            if callback.best_model_path and callback.best_model_path != "":
                                ckpt_path = callback.best_model_path
                                selected_ckpt_type = "loss"
                                log.info(f"Using checkpoint from {test_ckpt_type} callback: {ckpt_path}")
                                break
                        elif test_ckpt_type == "pcc" and "pcc" in str(callback.monitor).lower():
                            if callback.best_model_path and callback.best_model_path != "":
                                ckpt_path = callback.best_model_path
                                selected_ckpt_type = "pcc"
                                log.info(f"Using checkpoint from {test_ckpt_type} callback: {ckpt_path}")
                                break
                
                # Fallback to first checkpoint callback if specific one not found
                if ckpt_path is None:
                    for callback in callbacks:
                        if isinstance(callback, L.pytorch.callbacks.ModelCheckpoint):
                            if callback.best_model_path and callback.best_model_path != "":
                                ckpt_path = callback.best_model_path
                                # Fallback checkpoint type when a specific monitor is not found
                                if test_ckpt_type in ["loss", "pcc"]:
                                    selected_ckpt_type = test_ckpt_type
                                else:
                                    selected_ckpt_type = "unknown"
                                log.info(f"Fallback: Using checkpoint: {ckpt_path}")
                                break
        else:
            ckpt_path = cfg.get("ckpt_path")

        # Pass the actually used checkpoint type to the model so that it can
        # append the suffix (e.g., predictions_loss.h5ad, predictions_pcc.h5ad).
        try:
            setattr(model, "current_test_ckpt_type", selected_ckpt_type)
        except Exception:
            # If anything goes wrong here, fall back to default behaviour
            # in which predictions are saved without a ckpt-type suffix.
            pass

        trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)
        summary_metrics_dict = model.summary_metrics.to_dict()[
            model.summary_metrics.columns[0]
        ]

    test_metrics = trainer.callback_metrics
    # merge train and test metrics
    metric_dict = {**train_metrics, **test_metrics, **summary_metrics_dict}

    return metric_dict


@hydra.main(version_base="1.3", config_path="../configs", config_name="train.yaml")
def main(cfg: DictConfig) -> float | None:
    global _PARSER_ARGS

    # CLI parser args have highest priority: use them to override Hydra cfg
    cfg = _apply_cli_overrides(cfg, _PARSER_ARGS)

    runtime_context = {"cfg": cfg, "trial_number": HydraConfig.get().job.get("num")}

    # Train the model
    global metric_dict
    metric_dict = train(runtime_context)

    # Combined metric
    metrics_use = cfg.get("metrics_to_optimize")
    if metrics_use:
        combined_metric = sum(
            [metric_dict.get(metric) * weight for metric, weight in metrics_use.items()]
        )
        return combined_metric


if __name__ == "__main__":
    # 1) First parse standard CLI flags (e.g. from wandb sweep), keep unknowns
    parser = _build_arg_parser()
    _PARSER_ARGS, remaining = parser.parse_known_args()

    # 2) Translate certain argparse flags into Hydra-style overrides so that
    #    config groups (data=..., model=..., logger=...) and complex types
    #    (like trainer.devices=[0]) are handled by Hydra, while leaf parameters
    #    remain as --key flags for our own override logic.
    hydra_overrides: list[str] = []

    if getattr(_PARSER_ARGS, "data", None) is not None:
        hydra_overrides.append(f"data={_PARSER_ARGS.data}")
        _PARSER_ARGS.data = None  # avoid double-handling

    if getattr(_PARSER_ARGS, "model", None) is not None:
        hydra_overrides.append(f"model={_PARSER_ARGS.model}")
        _PARSER_ARGS.model = None

    if getattr(_PARSER_ARGS, "logger", None) is not None:
        hydra_overrides.append(f"logger={_PARSER_ARGS.logger}")
        _PARSER_ARGS.logger = None

    # Handle trainer.devices specially so that Hydra, not Lightning, parses
    # the list syntax (e.g., [0]) correctly.
    if getattr(_PARSER_ARGS, "trainer_devices", None) is not None:
        hydra_overrides.append(f"trainer.devices={_PARSER_ARGS.trainer_devices}")
        _PARSER_ARGS.trainer_devices = None

    # 3) Let Hydra see its overrides plus the remaining arguments
    sys.argv = [sys.argv[0]] + hydra_overrides + remaining

    # 4) Run Hydra entrypoint as usual; inside main we will override cfg with _PARSER_ARGS
    main()
